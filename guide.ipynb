{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this guide we will design and train an image recognition model for recognizing 17 different species of flowers. We will use a technique called transfer learning, combining a predefined model called VGG19 trained on ImageNet with our own flower classification subnet. The guide is an extension of the workshop found [here](https://www.tekna.no/kurs/workshop-introduksjon-til-maskinlaring-48133/), and thus requires some shallow understanding of machine learning theory and programming in Python. Running the code has two prerequisites: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A python environment containing tensorflow:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Downloading and restructuring the dataset in a folder 'flowers' as defined [here](https://github.com/estenhl/flowers/blob/master/README.md):__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print('Root dir exists: {}'.format(os.path.isdir('flowers')))\n",
    "print('Train dir exists: {}'.format(\n",
    "      os.path.isdir(os.path.join('flowers', 'train'))))\n",
    "print('Validation dir exists: {}'.format(\n",
    "      os.path.isdir(os.path.join('flowers', 'val'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Serving images in-memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To serve images to our model during training we will use a python Generator. Given that our images are structured as previously stated, a folder with the training set and a folder with the validation set, both with subfolders for each category of flowers, we can use a tensorflow dataset instantiated with [image_dataset_from_directory](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image_dataset_from_directory). The generator acts as a list and serves what is known as batches of tuples, where the first element of the tuple contains images and the second element contains the corresponding labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "batches = image_dataset_from_directory('flowers/train', batch_size=4)\n",
    "\n",
    "batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point it is usually a good idea to do a [sanity check](https://en.wiktionary.org/wiki/sanity_check). This typically includes verifying that our images are served on the correct format, and that the images and labels are still correctly matched. We can first fetch the reverse encoding of the generator to be able to understand what the numeric labels encode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = batches.class_names\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use matplotlib to visualize the first batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "for X, y in batches:\n",
    "    fig, ax = plt.subplots(1, 4, figsize=(10, 10))\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        img = X[i].numpy()\n",
    "        img = img.astype(np.uint8)\n",
    "        label = labels[y[i]]\n",
    "\n",
    "        ax[i].imshow(img)\n",
    "        ax[i].set_title(label)\n",
    "        ax[i].set_xticks([])\n",
    "        ax[i].set_yticks([])\n",
    "\n",
    "    plt.show()\n",
    "    break # We only need the first batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Setting up the base model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we know our dataset is being served correctly we can start setting up the base model that will be the core of our flower classification model. As previously mentioned this will be a model called VGG19, a small model which yields relatively good results. Like the generator, this also exists as a prebuilt module in Keras, namely in the [applications-module](https://keras.io/applications/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg19 import VGG19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When initializing the model we need to specify that we want to use the weights trained on ImageNet, that we want the entire model including top layers, and we also specify the image size we are going to use for verbosity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG19(weights='imagenet', include_top=True,\n",
    "              input_shape=(224, 224, 3))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can sanity check this step by running predicting the label for an image from our generator. Note that the predictions we are doing now will be using the labels from ImageNet, as this is what the model currently recognizes, not the labels from our dataset. We start by reinitializing the generator with the correct image size and a batch size of 1. We also set the seed for the random library to control the order of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "generator = image_dataset_from_directory('flowers/train', batch_size=1,\n",
    "                                         image_size=(224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then procure predictions from the first batch containing a single image. To decode the prediction using imagenet labels we can use a predefined function found in the same module as the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg19 import decode_predictions\n",
    "\n",
    "for X, y in generator:\n",
    "    preds = model.predict(X)\n",
    "    decoded_preds = decode_predictions(preds, top=1)\n",
    "    fig = plt.figure()\n",
    "\n",
    "    img = X[0].numpy()\n",
    "    img = img.astype(np.uint8)\n",
    "    label = labels[y[0]]\n",
    "    predicted = decoded_preds[0]\n",
    "\n",
    "    plt.imshow(img)\n",
    "    fig.suptitle('Truth: {}, Predicted: {}'.format(label, predicted))\n",
    "    plt.show()\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know both our generator and model are set up, and we are able to make predictions. The predictions, however, does not necessarily look very good. This is because of a process called preprocessing: A set of transformations applied to the images before training to give the model the best possible foundation to learn what it needs. Typical preprocessing includes rescaling the values of the data, shifting the range of pixel-values, and similar numerical operations. Luckily, in Keras, the module which contains a model also contains the preprocessing function used for training the model. We can fetch this function and feed it to our generator to ensure all images are preprocessed before they are served to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
    "\n",
    "np.random.seed(42)\n",
    "generator = image_dataset_from_directory('flowers/train', batch_size=1,\n",
    "                                         image_size=(224, 224))\n",
    "generator = generator.map(lambda images, labels: (preprocess_input(images),\n",
    "                                                  labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have reinitialized the generator correctly we can rerun our predictions to see if they improve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, y in generator:\n",
    "    preds = model.predict(X)\n",
    "    decoded_preds = decode_predictions(preds, top=1)\n",
    "    fig = plt.figure()\n",
    "\n",
    "    img = X[0].numpy()\n",
    "    img = img.astype(np.uint8)\n",
    "    label = labels[y[0]]\n",
    "    predicted = decoded_preds[0]\n",
    "\n",
    "    plt.imshow(img)\n",
    "    fig.suptitle('Truth: {}, Predicted: {}'.format(label, predicted))\n",
    "    plt.show()\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that seeing improvements in the predictions is not a given even though the images are now preprocessed correctly. The label we are looking for might not be a part of the original dataset the model was trained on, or it might simply be a case of a bad prediction where the model misses. However, running a sanity check (preferably over more images) is usually a good habit to ensure correctness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Configuring the flower classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we are happy with the behaviour of our base model, we can start setting up our own custom model for solving the problem we are interested in, in our case classifying flowers. The first step is to be a bit more restrictive with what we use from the pretrained model, only picking out the parts we need. We do this by dropping the top layers used for predictions, and instead perform a pooling operation on the final convolutional layer. Once initialized, we can fetch the input and output of the pretrained model using properties found in keras' model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = VGG19(include_top=False, input_shape=(224, 224, 3),\n",
    "                   weights='imagenet', pooling='max')\n",
    "inputs = pretrained.input\n",
    "outputs = pretrained.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we do not want the weights in this part of the final model to change, we freeze them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in pretrained.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then create our own custom layers for performing our own task. To start, we will use a hidden fully connected layer with 128 neurons, and a final prediction layer with 17 neurons, one per specie in our dataset. Note that the hidden layer takes the output from the pretrained model as its input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "hidden = Dense(128, activation='relu')(outputs)\n",
    "preds = Dense(17, activation='softmax')(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have all our layers set up we can wrap them in a Model, and compile the model using a pretty standardized set of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Model(inputs, preds)\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=Adam(learning_rate=1e-3),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then set up generators like we did before, one for the training data and one for validation, and train the model using Model.fit(). Note that training here is set to ten epochs for demonstration purposes, and more epochs might be necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_generator = image_dataset_from_directory('flowers/train',\n",
    "                                               batch_size=batch_size,\n",
    "                                               image_size=(224, 224))\n",
    "train_generator = train_generator.map(\n",
    "    lambda images, labels: (preprocess_input(images), labels)\n",
    "  )\n",
    "\n",
    "val_generator = image_dataset_from_directory('flowers/val',\n",
    "                                             batch_size=batch_size,\n",
    "                                             image_size=(224, 224))\n",
    "val_generator = val_generator.map(\n",
    "    lambda images, labels: (preprocess_input(images), labels)\n",
    "  )\n",
    "\n",
    "model.fit(train_generator,\n",
    "          epochs=10,\n",
    "          validation_data=val_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then predict using the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "for X, y in val_generator:\n",
    "    preds = model.predict(X)\n",
    "\n",
    "    truth = labels[y[0]]\n",
    "    label = labels[np.argmax(preds[0])]\n",
    "    probability = preds[0][np.argmax(preds[0])]\n",
    "    fig = plt.figure()\n",
    "\n",
    "    img = X[0].numpy()\n",
    "    img = img.astype(np.uint8)\n",
    "\n",
    "    plt.imshow(img)\n",
    "    fig.suptitle('Predicted: {}, probability: {}, truth: {}'.format(\n",
    "        label, probability, truth)\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the training above for a larger number of epochs, you will typically achieve a relatively decent result on the training data and a considerably worse outcome on the validation data (I see 100% training accuracy and about 65% validation accuracy, but this is prone to varying based on various randomness in the process). This is an example of overfitting: The model starts remembering specifics from the training set instead of learning general features that also generalize to new data. We handle this by introducing regularization, a common technique for forcing the model to generalize. In image recognition this is typically done using dropout-layers, that randomly drops a subset of its artificial neurons while training. We can introduce dropout to our model by inserting a Dropout layer which drops 30% of the neurons between the two final layers of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "hidden = Dense(128, activation='relu')(outputs)\n",
    "dropout = Dropout(.3)(hidden)\n",
    "preds = Dense(17, activation='softmax')(dropout)\n",
    "\n",
    "model = Model(inputs, preds)\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=Adam(learning_rate=1e-3),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can recompile the model and restart training to achieve what should be a better result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "train_generator = image_dataset_from_directory('flowers/train', batch_size=batch_size,\n",
    "                                               image_size=(224, 224))\n",
    "train_generator = train_generator.map(\n",
    "    lambda images, labels: (preprocess_input(images), labels)\n",
    "  )\n",
    "\n",
    "val_generator = image_dataset_from_directory('flowers/val', batch_size=batch_size,\n",
    "                                             image_size=(224, 224))\n",
    "val_generator = val_generator.map(\n",
    "    lambda images, labels: (preprocess_input(images), labels)\n",
    "  )\n",
    "\n",
    "model.fit(train_generator,\n",
    "          epochs=10,\n",
    "          validation_data=val_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A second technique for avoiding overfitting is augmenting the images, which goal it is to take the existing data points in our dataset and create brand new samples. This process works by somehow modifying an image in a way which changes it, while maintaining the thruthfulness of the corresponding label. An example in our case is mirroring the images vertically, which can be implemented with the map-function of our dataset. Using this functionality will randomly decide whether to flip the image or not each time the image is presented, theoretically yielding two samples from the single data point we started with. We can see this by visualizing the same image served from multiple batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import expand_dims, Tensor\n",
    "from tensorflow.image import random_flip_left_right\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 5, figsize=(15, 10))\n",
    "\n",
    "\n",
    "def augment(images: Tensor, labels: Tensor) -> Tuple[Tensor]:\n",
    "  first_image = images[0]\n",
    "  first_image = random_flip_left_right(first_image)\n",
    "  image_batch = expand_dims(first_image, 0)\n",
    "\n",
    "  return image_batch, labels\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    np.random.seed(42)\n",
    "    generator = image_dataset_from_directory('flowers/train', batch_size=1,\n",
    "                                             seed=42,\n",
    "                                             image_size=(224, 224))\n",
    "    generator = generator.map(augment)\n",
    "\n",
    "    for X, y in generator:\n",
    "        img = X[0].numpy()\n",
    "        img = img.astype(np.uint8)\n",
    "        ax[i].imshow(img)\n",
    "        ax[i].set_title('Run {}'.format(i + 1))\n",
    "        ax[i].set_xticks([])\n",
    "        ax[i].set_yticks([])\n",
    "        break\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retraining the model with a set of augmentations should increase the accuracy even further. There exists a variety of options, and getting the best performance can sometime feel more like an art than a science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps in this guide provide a good starting point for classifying species of flowers, or solving any other generic image classification problem. Retracing the steps while leaving more epochs for the model to train should provide a solid baseline with decent results (my best run achieved ~75% validation accuracy). Continued work on this problem would typically include trying different architectures as core models, experimenting with various designs for the custom problem-specific final layers and testing a wide range of combinations of regularization and augmentations to combat overfitting. It should be a feasible goal to reach an accuracy in the high 90s, which seem to be how the state-of-the-art models are performing. Happy hacking!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
