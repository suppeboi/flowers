{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this guide we will design and train an image recognition model for recognizing 17 different species of flowers. We will use a technique called transfer learning, combining a predefined model called VGG19 trained on ImageNet with our own flower classification subnet. The guide is an extension of the workshop found [here](https://www.tekna.no/kurs/maskinlaringsworkshop---python-36454/), and thus requires some shallow understanding of machine learning theory and programming in Python. Running the code has two prerequisites: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__A python environment containing tensorflow:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Downloading and restructuring the dataset in a folder 'flowers' as defined [here](https://github.com/epimedai/flowers/blob/master/README.md):__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print('Root dir exists: {}'.format(os.path.isdir('flowers')))\n",
    "print('Train dir exists: {}'.format(\n",
    "      os.path.isdir(os.path.join('flowers', 'train'))))\n",
    "print('Validation dir exists: {}'.format(\n",
    "      os.path.isdir(os.path.join('flowers', 'val'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Serving images in-memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To serve images to our model during training we will use a python Generator. Given that our images are structured as previously stated, a folder with the training set and a folder with the validation set, both with subfolders for each category of flowers, we can use a prebuilt keras generator called [ImageDataGenerator](https://keras.io/preprocessing/image/). The generator acts as a list and serves what is known as batches of tuples, where the first element of the tuple contains the images and of the batch and the second element contains the corresponding labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "batches = image_dataset_from_directory('flowers/train', batch_size=4)\n",
    "\n",
    "batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point it is usually a good idea to do a [sanity check](https://en.wiktionary.org/wiki/sanity_check). This typically includes verifying that our images are served on the correct format, and that the images and labels are still correctly matched. We can first fetch the reverse encoding of the generator to be able to decode the onehot encoded labels given in the batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = batches.class_names\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use matplotlib to visualize the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "for X, y in batches:\n",
    "    fig, ax = plt.subplots(1, 4, figsize=(10, 10))\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        img = X[i].numpy()\n",
    "        img = img.astype(np.uint8)\n",
    "        label = labels[y[i]]\n",
    "\n",
    "        ax[i].imshow(img)\n",
    "        ax[i].set_title(label)\n",
    "        ax[i].set_xticks([])\n",
    "        ax[i].set_yticks([])\n",
    "\n",
    "    plt.show()\n",
    "    break # We only need the first batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Setting up the base model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we know our dataset is being served correctly we can start setting up the base model that will be the core of our flower classification model. As previously mentioned this will be a model called VGG19, a small model which yields relatively good results. Like the generator, this also exists as a prebuilt module in Keras, namely in the [applications-module](https://keras.io/applications/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg19 import VGG19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When initializing the model we need to specify that we want to use the weights trained on ImageNet, that we want the entire model including top layers, and we also specify the image size we are going to use for verbosity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG19(weights='imagenet', include_top=True,\n",
    "              input_shape=(224, 224, 3))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can sanity check this step by running predicting the label for an image from our generator. Note that the predictions we are doing now will be using the labels from ImageNet, as this is what the model currently recognizes, not the labels from our dataset. We start by reinitializing the generator with the correct size and a batch size of 1. We also set the seed for the random library to control the order of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "generator = image_dataset_from_directory('flowers/train', batch_size=1,\n",
    "                                         image_size=(224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then run predictions on the first batch containing a single image. To decode the prediction using imagenet labels we can use a predefined function found in the same module as the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg19 import decode_predictions\n",
    "\n",
    "for X, y in generator:\n",
    "    preds = model.predict(X)\n",
    "    decoded_preds = decode_predictions(preds, top=1)\n",
    "    fig = plt.figure()\n",
    "\n",
    "    img = X[0].numpy()\n",
    "    img = img.astype(np.uint8)\n",
    "    label = labels[y[0]]\n",
    "    predicted = decoded_preds[0]\n",
    "\n",
    "    plt.imshow(img)\n",
    "    fig.suptitle('Truth: {}, Predicted: {}'.format(label, predicted))\n",
    "    plt.show()\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know both our generator and model are set up, and we are able to make predictions. The predictions, however, does not necessarily look very good. This is because of a process called preprocessing: A set of transformations applied to the images before training to give the model the best possible foundation to learn what it needs. Typical preprocessing includes rescaling the values of the data, shifting the range, and other numerical operations. Luckily, in Keras, the module which contains a model also contains the preprocessing function used for training the model. We can fetch this function and feed it to our generator to ensure all images are preprocessed before they are served to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
    "\n",
    "np.random.seed(42)\n",
    "generator = image_dataset_from_directory('flowers/train', batch_size=1,\n",
    "                                         image_size=(224, 224))\n",
    "generator = generator.map(lambda images, labels: (preprocess_input(images),\n",
    "                                                  labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have reinitialized the generator correctly we can rerun our predictions to see if they improve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X, y in generator:\n",
    "    preds = model.predict(X)\n",
    "    decoded_preds = decode_predictions(preds, top=1)\n",
    "    fig = plt.figure()\n",
    "\n",
    "    img = X[0].numpy()\n",
    "    img = img.astype(np.uint8)\n",
    "    label = labels[y[0]]\n",
    "    predicted = decoded_preds[0]\n",
    "\n",
    "    plt.imshow(img)\n",
    "    fig.suptitle('Truth: {}, Predicted: {}'.format(label, predicted))\n",
    "    plt.show()\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that seeing improvements in the predictions is not a given even though the images are now preprocessed correctly. The label we are looking for might not be a part of the original dataset the model was trained on, or it might simply be a case of a bad prediction where the model misses. However, running a sanity check (preferably over more images) is usually a good habit to achieve the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Configuring the flower classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we are happy with the interaction of our base model, we can start setting up our own custom model for solving the problem we are interested in, in this case classifying flower species. The first step is to be a bit more restrictive with what we use from the pretrained model, only picking out the parts we need. We do this by dropping the top layers used for predictions, and instead perform a pooling operation on the final convolutional layer. Once we have initialized it we can fetch the input and the output of the pretrained model using properties found in keras' model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained = VGG19(include_top=False, input_shape=(224, 224, 3),\n",
    "                   weights='imagenet', pooling='max')\n",
    "inputs = pretrained.input\n",
    "outputs = pretrained.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we do not want the weights in this part of the final model to change, we can freeze them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in pretrained.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then create our own custom layers for performing our own task. We will use a hidden fully connected layer with 128 neurons, and a final prediction layer with 17 neurons, one per specie in our dataset. Note that the hidden layer takes the output from the pretrained model as its input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "hidden = Dense(128, activation='relu')(outputs)\n",
    "preds = Dense(17, activation='softmax')(hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have all our layers set up we can wrap them in a Model, and compile the model using a pretty standardized set of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "model = Model(inputs, preds)\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=Adam(learning_rate=1e-3),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then set up generators like we did before, one for the training data and one for validation, and train the model using fit_generator. Note that training here is set to a single epoch simply for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_generator = image_dataset_from_directory('flowers/train',\n",
    "                                               batch_size=batch_size,\n",
    "                                               image_size=(224, 224))\n",
    "train_generator = train_generator.map(\n",
    "    lambda images, labels: (preprocess_input(images), labels)\n",
    "  )\n",
    "\n",
    "val_generator = image_dataset_from_directory('flowers/val',\n",
    "                                             batch_size=batch_size,\n",
    "                                             image_size=(224, 224))\n",
    "val_generator = val_generator.map(\n",
    "    lambda images, labels: (preprocess_input(images), labels)\n",
    "  )\n",
    "\n",
    "model.fit(train_generator,\n",
    "          epochs=10,\n",
    "          validation_data=val_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We predict using the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "for X, y in val_generator:\n",
    "    preds = model.predict(X)\n",
    "\n",
    "    truth = labels[y[0]]\n",
    "    label = labels[np.argmax(preds[0])]\n",
    "    probability = preds[0][np.argmax(preds[0])]\n",
    "    fig = plt.figure()\n",
    "\n",
    "    img = X[0].numpy()\n",
    "    img = img.astype(np.uint8)\n",
    "\n",
    "    plt.imshow(img)\n",
    "    fig.suptitle('Predicted: {}, probability: {}, truth: {}'.format(\n",
    "        label, probability, truth)\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run the training above for a larger number of epochs, you will typically achieve a very decent result on the training data and a considerably worse outcome on the validation data. This is an example of overfitting: The model starts remembering specifics from the training set instead of learning the general features we are interested in. We handle this by introducing regularization, trying to force the model to generalize. In image recognition this is typically done using dropout-layers, which during training randomly sets the firing of a subset of neurons to 0. We can introduce dropout to our model by inserting a Dropout layer which drops 30% of the neurons between the two final layers of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "hidden = Dense(128, activation='relu')(outputs)\n",
    "dropout = Dropout(.3)(hidden)\n",
    "preds = Dense(17, activation='softmax')(dropout)\n",
    "\n",
    "model = Model(inputs, preds)\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer=Adam(learning_rate=1e-3),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can recompile the model and restart training to achieve what should be a better result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "train_generator = image_dataset_from_directory('flowers/train', batch_size=batch_size,\n",
    "                                               image_size=(224, 224))\n",
    "train_generator = train_generator.map(\n",
    "    lambda images, labels: (preprocess_input(images), labels)\n",
    "  )\n",
    "\n",
    "val_generator = image_dataset_from_directory('flowers/val', batch_size=batch_size,\n",
    "                                             image_size=(224, 224))\n",
    "val_generator = val_generator.map(\n",
    "    lambda images, labels: (preprocess_input(images), labels)\n",
    "  )\n",
    "\n",
    "model.fit(train_generator,\n",
    "          epochs=10,\n",
    "          validation_data=val_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A second technique for avoiding overfitting is augmenting the images, which goal it is to take the existing data points in our dataset and create brand new samples. It works by somehow modifying an image in a way which changes it, while maintaining the thruthfulness of the corresponding label. An example in our case is mirroring the images vertically, which can be implemented by using the map-function of the dataset. Using this functionality will randomly decide whether to flip the image or not each time the image is presented, theoretically yielding two samples from the single data point we started with. We can see this by visualizing the same image served from multiple batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import expand_dims, Tensor\n",
    "from tensorflow.image import random_flip_left_right\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1, 5, figsize=(15, 10))\n",
    "\n",
    "\n",
    "def augment(images: Tensor, labels: Tensor) -> Tuple[Tensor]:\n",
    "  first_image = images[0]\n",
    "  first_image = random_flip_left_right(first_image)\n",
    "  image_batch = expand_dims(first_image, 0)\n",
    "\n",
    "  return image_batch, labels\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    np.random.seed(42)\n",
    "    generator = image_dataset_from_directory('flowers/train', batch_size=1,\n",
    "                                             seed=42,\n",
    "                                             image_size=(224, 224))\n",
    "    generator = generator.map(augment)\n",
    "\n",
    "    for X, y in generator:\n",
    "        img = X[0].numpy()\n",
    "        img = img.astype(np.uint8)\n",
    "        ax[i].imshow(img)\n",
    "        ax[i].set_title('Run {}'.format(i + 1))\n",
    "        ax[i].set_xticks([])\n",
    "        ax[i].set_yticks([])\n",
    "        break\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retraining the model with a set of augmentations should increase the accuracy even further. From now on, you can experiment with various parameters given to the [ImageDataGenerator class](https://keras.io/preprocessing/image/), and train the model by your self."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps in this guide provide a good starting point for classifying species of flowers, or solving any other generic image classification problem. Retracing the steps while leaving more epochs for the model to train should provide a solid baseline with decent results (my best run achieved ~75%). Continued work on this problem would typically include trying different architectures as core models, experimenting with various designs for the custom problem-specific final layers and testing a wide range of combinations of regularization and augmentations to combat overfitting. It should be a feasible goal to reach an accuracy in the high 90s, which seem to be how the state-of-the-art models are performing. Happy hacking!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
